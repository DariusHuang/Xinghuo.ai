{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian Optimization with GPs & EI for ResNet50\n",
    "###### 黄思铭 2020/10/20\n",
    "##### 步骤：\n",
    "1. 搭建迁移学习模型与数据管道\n",
    "2. 定义核心贝叶斯优化函数  \n",
    "    - Regression Model: Gaussian Processes\n",
    "    - Acquisition Function: Expected-Improvement(EI)\n",
    "3. 获取数据集，获取初始采集点\n",
    "4. 迭代并获取最佳X值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import load_model\n",
    "import math\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "#使用了Matern Kernel作为回归模型的核\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel\n",
    "from scipy.optimize import minimize\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. 搭建迁移学习与数据管道"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "初始化全局变量\n",
    "'''\n",
    "IMG_WIDTH = 128\n",
    "IMG_HEIGHT = 128\n",
    "IMG_CHANNEL = 3\n",
    "MARK_SIZE = 32 #特征点个数\n",
    "\n",
    "EPOCHS=1\n",
    "BATCH_SIZE=32\n",
    "\n",
    "class BestLogger(keras.callbacks.Callback):\n",
    "    '''\n",
    "    回调函数\n",
    "    '''\n",
    "    def __init__(self, file):\n",
    "        super(BestLogger, self).__init__()\n",
    "        self.file = file\n",
    "        self.best = float(\"inf\")\n",
    "\n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        loss = logs.get('val_loss')\n",
    "        acc = logs.get('val_acc')\n",
    "        if math.isinf(self.best) or loss < self.best:\n",
    "            self.best = loss\n",
    "            with open(self.file, \"w\") as text_file:\n",
    "                obj = {\n",
    "                    'batch' : batch,\n",
    "                    'val_loss' : loss,\n",
    "                    'val_acc' : acc\n",
    "                }\n",
    "                print(json.dumps(obj), text_file)\n",
    "                \n",
    "\n",
    "def get_compiled_model(output_size, learning_rate):\n",
    "    '''\n",
    "    定义模型，将learning_rate作为参数\n",
    "    '''\n",
    "    from tensorflow.keras.applications.resnet50  import ResNet50\n",
    "    from tensorflow.keras.models import load_model, Model\n",
    "    from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "      \n",
    "    base_model = ResNet50(input_shape=(128,128,3),\n",
    "                           include_top=False,\n",
    "                           # weights=\"imagenet\",\n",
    "                           weights='resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5',\n",
    "#                            alpha=1.0,\n",
    "#                            dropout=1e-3)\n",
    "                         )\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    predictions = Dense(64)(x)\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(lr=learning_rate)\n",
    "    \n",
    "    # 不能使用Accuracy作为衡量指标                   # keras.metrics.mean_squared_error\n",
    "    model.compile(optimizer=optimizer, metrics=[keras.metrics.mean_squared_error], loss=keras.losses.mean_squared_error)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def _parse(example):\n",
    "    \"\"\"从tfrecord文件中提取训练数据\n",
    "    Args:\n",
    "        example: protobuf文件.\n",
    "\n",
    "    Returns:\n",
    "        data-label对.\n",
    "    \"\"\"\n",
    "    keys_to_features = {\n",
    "        'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "        'label/marks': tf.io.FixedLenFeature([64], tf.float32),\n",
    "    }\n",
    "    parsed_features = tf.io.parse_single_example(example, keys_to_features)\n",
    "\n",
    "    # Extract features from single example\n",
    "    image_decoded = tf.image.decode_image(parsed_features['image/encoded'])\n",
    "    image_reshaped = tf.reshape(\n",
    "        image_decoded, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNEL])\n",
    "    image_float = tf.cast(image_reshaped, tf.float32)\n",
    "    points = tf.cast(parsed_features['label/marks'], tf.float32)\n",
    "\n",
    "    return image_float, points\n",
    "\n",
    "\n",
    "def get_parsed_dataset(record_file, batch_size, epochs=None, shuffle=True):\n",
    "    \"\"\"返回tf.dataset以供训练\n",
    "    Args:\n",
    "        record_file: TFRecord file.\n",
    "        batch_size: batch size.\n",
    "        epochs: epochs of dataset.\n",
    "        shuffle: whether to shuffle the data.\n",
    "\n",
    "    Returns:\n",
    "        Pparsed tf.dataset.\n",
    "    \"\"\"\n",
    "    dataset = tf.data.TFRecordDataset(record_file)\n",
    "\n",
    "    if shuffle is True:\n",
    "        dataset = dataset.shuffle(buffer_size=10000)\n",
    "    dataset = dataset.map(\n",
    "        _parse, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.repeat(epochs)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def run():\n",
    "    \"\"\"训练与储存模型\"\"\"\n",
    "\n",
    "    # Create the Model\n",
    "    mark_model = get_compiled_model(MARK_SIZE*2)\n",
    "\n",
    "    callbacks = [keras.callbacks.TensorBoard(log_dir='./log', update_freq=1024),\n",
    "#                  BestLogger('/home/tione/log/best.json'),\n",
    "                 keras.callbacks.ModelCheckpoint(filepath='./train',\n",
    "                                                 monitor='loss',\n",
    "                                                 save_freq='epoch')]\n",
    "    if True:\n",
    "        train_dataset = get_parsed_dataset(record_file='./tfrecord/train.record',\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           epochs=EPOCHS,\n",
    "                                           shuffle=True)\n",
    "        print('Starting to train.')\n",
    "        _ = mark_model.fit(train_dataset,\n",
    "                           epochs=EPOCHS,\n",
    "#                            steps_per_epoch=TRAINING_STEPS,\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "    if True:\n",
    "        print('Starting to evaluate.')\n",
    "        eval_dataset = get_parsed_dataset(record_file=\"./tfrecord/validation.record\",\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          epochs=1,\n",
    "                                          shuffle=False)\n",
    "        evaluation = mark_model.evaluate(eval_dataset)\n",
    "        print(evaluation)\n",
    "\n",
    "    if True:\n",
    "        #print(\"Saving model to directory: {}\".format(\"/home/tione/saved_model1\"))\n",
    "        mark_model.save(\"saved_resnet.h5\")\n",
    "        #mark_model.save_weights('/home/tione/saved_model/easy_checkpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "!pip install pydot\n",
    "!pip install pydotplus\n",
    "!apt-get install graphviz\n",
    "!pip install graphviz\n",
    "handout = get_compiled_model(0.001,MARK_SIZE*2)\n",
    "keras.utils.plot_model(handout, \"resnet50.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. 定义优化函数（选择回归模型，定义黑盒函数, 定义acquisition函数）    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黑盒函数\n",
    "def blackbox(train_dataset,tuned_learning_rate):\n",
    "    \"\"\"\n",
    "    Attribute: Resource-expensive\n",
    "    \n",
    "    Arguments:\n",
    "    x -- image data.\n",
    "    y -- labels.\n",
    "    learning_rate -- learning rate为超参数\n",
    "\n",
    "    Returns:\n",
    "    accuracy of the trained model under selected learning rate.\n",
    "    \"\"\"\n",
    "    epochs = 1\n",
    "    model = get_compiled_model(MARK_SIZE*2, tuned_learning_rate)\n",
    "\n",
    "    # 训练在tuned_learning_rate下的模型\n",
    "    trained_model_history = model.fit(train_dataset,\n",
    "                         epochs=epochs,\n",
    "                         verbose=0\n",
    "                         )\n",
    "    \n",
    "    loss = trained_model_history.history['loss'][-1]\n",
    "    print(loss)\n",
    "    \n",
    "    # 删除模型\n",
    "    del model\n",
    "\n",
    "    # 删除session，详见：https://www.coder.work/article/97730\n",
    "    K.clear_session()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 返回在X处的expected improvement\n",
    "def expected_improvement(X, X_sample, Y_sample, gpr, xi=0.0001): # xi=0.01\n",
    "    '''\n",
    "    Computes the EI at points X based on existing samples X_sample\n",
    "    and Y_sample using a Gaussian process surrogate model. \n",
    "    注: EI is Expected Improvement; GP is Gaussian process.\n",
    "\n",
    "    Arguments:\n",
    "        X -- Points at which EI shall be computed.\n",
    "        X_sample -- Sample locations.\n",
    "        Y_sample -- Sample values.\n",
    "        gpr -- A GaussianProcessRegressor fitted to samples.\n",
    "        xi -- Exploitation-exploration trade-off parameter.\n",
    "\n",
    "    Returns:\n",
    "        Expected improvements at points X.\n",
    "    '''\n",
    "\n",
    "    # 基于上一轮的高斯过程拟合，计算所有点的均值和标准差\n",
    "    mu, sigma = gpr.predict(X, return_std=True)\n",
    "\n",
    "    sigma = sigma.reshape(-1, 1)\n",
    "\n",
    "    # 计算最低loss\n",
    "    sample_opt = np.min(Y_sample)\n",
    "\n",
    "    # 用errstate防止计算问题而崩溃\n",
    "    with np.errstate(divide='warn'):\n",
    "        imp = mu - sample_opt - xi\n",
    "        Z = imp / sigma\n",
    "        ei = imp * norm.cdf(Z) + sigma * norm.pdf(Z)\n",
    "        ei[sigma == 0.0] = 0.0\n",
    "\n",
    "    return ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于现有猜测集合，预计下一个最佳X值的坐标\n",
    "def sample_next_hyperparameter(acquisition, X_sample, Y_sample, gpr, bounds, n_restarts=50):\n",
    "    '''\n",
    "    Proposes the next hyperparameter point by optimizing\n",
    "    the acquisition function.\n",
    "\n",
    "    Args:\n",
    "        acquisition -- Acquisition function.\n",
    "        X_sample -- Sample locations.\n",
    "        Y_sample -- Sample values.\n",
    "        gpr -- A GaussianProcessRegressor fitted to samples.\n",
    "        bounds -- Bounds of the hyperparameter.\n",
    "        n_restarts -- Number of restarts for finding the optimum.\n",
    "\n",
    "    Returns:\n",
    "        Location of the acquisition function maximum.\n",
    "    '''\n",
    "\n",
    "    dim = X_sample.shape[1]\n",
    "    min_val = float('inf')\n",
    "    min_x = None\n",
    "\n",
    "    def min_obj(X):\n",
    "        # Minimization objective is the negative acquisition function\n",
    "        return -acquisition(X.reshape(-1, dim), X_sample, Y_sample, gpr)\n",
    "\n",
    "    # Find the best optimum by starting from n_restart different random points.\n",
    "    for x0 in np.random.uniform(\n",
    "            bounds[:, 0], bounds[:, 1], size=(n_restarts, dim)):\n",
    "        res = minimize(min_obj, x0=x0, bounds=bounds, method='L-BFGS-B')\n",
    "        if res.fun < min_val:\n",
    "            min_val = res.fun[0]\n",
    "            min_x = res.x\n",
    "\n",
    "    return min_x.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 获取Dataset，获取初始X值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<DatasetV1Adapter shapes: ((?, 128, 128, 3), (?, 64)), types: (tf.float32, tf.float32)>\n",
      "<DatasetV1Adapter shapes: ((?, 128, 128, 3), (?, 64)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "# 获取dataset\n",
    "train_dataset = get_parsed_dataset(record_file='./tfrecord/train.record',\n",
    "                                           batch_size=BATCH_SIZE,\n",
    "                                           epochs=EPOCHS,\n",
    "                                           shuffle=True)\n",
    "eval_dataset = get_parsed_dataset(record_file=\"./tfrecord/validation.record\",\n",
    "                                          batch_size=BATCH_SIZE,\n",
    "                                          epochs=1,\n",
    "                                          shuffle=False)\n",
    "print(train_dataset)\n",
    "print(eval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `batch_size` argument must not be specified for the given input type. Received input: <DatasetV1Adapter shapes: ((?, 128, 128, 3), (?, 64)), types: (tf.float32, tf.float32)>, batch_size: 32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mValueError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-12f8a79ed302>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2e-1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Testing only: Y_init = np.array([[98e-4],[77e-4]])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m Y_init = np.vstack((blackbox(train_dataset, X_init[0][0]),\n\u001b[0m\u001b[1;32m     10\u001b[0m                     blackbox(train_dataset, X_init[1][0])))\n\u001b[1;32m     11\u001b[0m \u001b[0mY_init\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-135-b8646a1948d7>\u001b[0m in \u001b[0;36mblackbox\u001b[0;34m(train_dataset, tuned_learning_rate)\u001b[0m\n\u001b[1;32m     20\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m                          \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m                          )\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    629\u001b[0m           **kwargs):\n\u001b[1;32m    630\u001b[0m     batch_size = model._validate_or_infer_batch_size(batch_size,\n\u001b[0;32m--> 631\u001b[0;31m                                                      steps_per_epoch, x)\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     x, y, sample_weights = model._standardize_user_data(\n",
      "\u001b[0;32m/opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages/tensorflow_core/python/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_validate_or_infer_batch_size\u001b[0;34m(self, batch_size, steps, x)\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;34m'The `batch_size` argument must not be specified for the given '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1814\u001b[0m             'input type. Received input: {}, batch_size: {}'.format(\n\u001b[0;32m-> 1815\u001b[0;31m                 x, batch_size))\n\u001b[0m\u001b[1;32m   1816\u001b[0m       \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The `batch_size` argument must not be specified for the given input type. Received input: <DatasetV1Adapter shapes: ((?, 128, 128, 3), (?, 64)), types: (tf.float32, tf.float32)>, batch_size: 32"
     ]
    }
   ],
   "source": [
    "# 获取初始X值与对应的Y值\n",
    "\n",
    "# bounds为学习速率可能范围\n",
    "bounds = np.array([[1e-5, 1]])\n",
    "\n",
    "#根据经验设置初始学习速率 获取对应Y值\n",
    "X_init = np.array([[1e-3], [2e-1]])\n",
    "# Testing only: Y_init = np.array([[98e-4],[77e-4]])\n",
    "Y_init = np.vstack((blackbox(train_dataset, X_init[0][0]),\n",
    "                    blackbox(train_dataset, X_init[1][0])))\n",
    "Y_init = np.array(Y_init).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 迭代并获取最佳X值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_init' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-5339b6358b88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Initialize samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mY_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Number of iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_init' is not defined"
     ]
    }
   ],
   "source": [
    "gpr_kernel = ConstantKernel(1.0) * Matern(length_scale=1, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=gpr_kernel)\n",
    "# Initialize samples\n",
    "X_sample = X_init\n",
    "Y_sample = Y_init\n",
    "\n",
    "# Number of iterations\n",
    "n_iter = 10\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # Update Gaussian process with existing samples\n",
    "    gpr.fit(X_sample, Y_sample)\n",
    "\n",
    "    # Obtain next sampling point from the acquisition function\n",
    "    # (expected_improvement)\n",
    "    X_next = sample_next_hyperparameter(\n",
    "        expected_improvement, X_sample, Y_sample, gpr, bounds)\n",
    "\n",
    "    # Obtain next noisy sample from the objective function\n",
    "    Y_next = blackbox(train_dataset, X_next[0][0])\n",
    "\n",
    "    # Add sample to previous samples\n",
    "    X_sample = np.vstack((X_sample, X_next))\n",
    "    Y_sample = np.vstack((Y_sample, Y_next))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sampled learning rates:\\n\", np.hstack((X_sample, Y_sample)))\n",
    "best_learning_rate = X_sample[Y_sample.argmax()][0]\n",
    "print(\"Best learning rate: \", best_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### source\n",
    "###### 1. https://github.com/ohquai/ResNet_cifar10_TensorFlow 对ResNet优化的启发\n",
    "###### 2. 参考了 https://github.com/thuijskens/bayesian-optimization 中高斯过程的代码\n",
    "###### 3. 重点参考了下文：https://static.sigopt.com/773979031a2d61595b9bda23bb81a192341f11a4/pdf/SigOpt_Bayesian_Optimization_Primer.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 候选方案：贝叶斯优化库实现超参数优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 黑盒函数\n",
    "def blackbox(learning_rate):\n",
    "    epochs = 3\n",
    "    model = get_compiled_model(MARK_SIZE*2, learning_rate)\n",
    "\n",
    "    blackbox = model.fit(train_dataset,epochs=epochs)\n",
    "    loss = blackbox.history['loss'][-1]\n",
    "\n",
    "    # delete the Keras model with these hyper-parameters from memory.\n",
    "    del model\n",
    "\n",
    "    # clear the keras session\n",
    "    K.clear_session()\n",
    "\n",
    "    return -loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip, can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support\u001b[0m\n",
      "Looking in indexes: http://mirrors.tencentyun.com/pypi/simple\n",
      "Requirement already satisfied: bayesian-optimization in /opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages (from bayesian-optimization) (1.16.5)\n",
      "Requirement already satisfied: scikit-learn>=0.18.0 in /opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages (from bayesian-optimization) (0.19.2)\n",
      "Requirement already satisfied: scipy>=0.14.0 in /opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages (from bayesian-optimization) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# 使用封装好的库\n",
    "! pip install bayesian-optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (domain_reduction.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/opt/conda/envs/tensorflow_py2/lib/python2.7/site-packages/bayes_opt/domain_reduction.py\"\u001b[0;36m, line \u001b[0;32m11\u001b[0m\n\u001b[0;31m    def initialize(self, target_space: TargetSpace):\u001b[0m\n\u001b[0m                                     ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BayesianOptimization' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-dba8afbea01b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpbounds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'learning_rate'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m optimizer = BayesianOptimization(f=blackbox,#导入黑盒算法\n\u001b[0m\u001b[1;32m      3\u001b[0m pbounds=pbounds,random_state=1)\n\u001b[1;32m      4\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#查看最优参数及表现\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BayesianOptimization' is not defined"
     ]
    }
   ],
   "source": [
    "pbounds = {'learning_rate': (1e-5, 1)}\n",
    "optimizer = BayesianOptimization(f=blackbox,#导入黑盒算法\n",
    "pbounds=pbounds,random_state=1)\n",
    "optimizer.maximize(init_points=2,n_iter=10)\n",
    "print(optimizer.max)#查看最优参数及表现\n",
    "for i, res in enumerate(optimizer.res):#查看其他参数及表现\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### source: https://github.com/fmfn/BayesianOptimization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_py2",
   "language": "python",
   "name": "conda_tensorflow_py2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
